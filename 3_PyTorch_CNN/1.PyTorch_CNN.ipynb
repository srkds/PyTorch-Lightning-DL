{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. CNN Theory\n",
        "\n",
        "### Difference between traditional feature extraction and CNN\n",
        "\n",
        "**Tabular Data**: each row represents the training examples and features are organized as columns. For example in iris dataset we have 4 different features.\n",
        "\n",
        "So here we want to work with image dataset.\n",
        "\n",
        "#### 1) Traditional Feature Extraction\n",
        "\n",
        "So in **traditional feature extraction** approach we **manually extract the features from the raw images**\n",
        "- See an `iris image` here we have `Petal` and `Sepal` dimentions.\n",
        "- We extract these features/measurements by applying a ruler and write down these numbers a feature vector.\n",
        "- So this would be manual feature extraction step  where we or someone else is taking these measurements.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Q5-swMsxabZJlxfAfrBEyRLD45QWqie0\" width=\"700\">\n",
        "\n",
        "#### 2) Mordern Deep Learning Architectures(CNN) for Feature Extraction\n",
        "- Here in CNN we dont have to worry about manual feature extrection.\n",
        "- Modern deep learning architechtures such CNN performs the feature extraction implicitly. Meaning automatically for us.\n",
        "- This allows us to feed image data to neural networks directly instead of the tabular data directly.\n",
        "\n",
        "#### Popular Examples of the CNNs\n",
        "1. Optical Charector Recognition software.\n",
        "2. Identifying different birds in your garden, we can implement a bird classifier using a CNN.\n",
        "\n",
        "### Features of CNNs\n",
        "- One of the **key feature of the convolutional neural networks** is **Convolutional layers**\n",
        "- **Convolutional layers** are feature extractors.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1npFjl79qS0hUaxTEZJdp1alItcxuhDLT\" width=\"700\">\n",
        "\n",
        "- Left most size 1st is the input image, typically that's an RGB image, RGB stands for Red, Green, and Blue.\n",
        "- We can think of RGB as stack of matrixes, So each matrix is a height and a width dimentions. and here we have 3 different stacked matrixes, the red matrix, green matrix, and blue matrix.\n",
        "- Each block in the figure referes the convolutional layer and there are 5 convolutional layers. and convolutional layers followed by fully connected layers. In the image we have 3 fully connected layers. 2 hidden layers and 1 output layer.\n",
        "- And we can think fully connected layer as a MLP. Typical Convolutional network architechture consists of a convolutional backbone and multilayer perceptron unit.\n",
        "- To draw a analogy with previous method(Manual feature extraction) see the below feature.\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1kPekc_xu1HOh-oPRYbDLor8mm8Y_jT4Q\" width=\"700\">\n",
        "\n",
        "- Previously in manual feature extraction, we **assume that someone has already extracted features in tabular format**.\n",
        "- Here we have an input image that goes through different convolutional layers to have a tabular like representation that then feeds into MLP part.\n",
        "- So here the new thing is here we have convolutional part where we have multiple convolutional layers.\n",
        "\n",
        "### Next, Why we exactly need convolutional layers?\n"
      ],
      "metadata": {
        "id": "JmYB9Gw-HZTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Image Data and Its Challenges\n",
        "\n",
        "### Question: can we use MLP on image data?\n",
        "**Ans:** is yes.\n",
        "\n",
        "- the gray scale image 14 X 14 pixels, and in order to use it in MLP we can concatenate rows in order to use it as a feature vector.\n",
        "- After concatanation we have 196 row vector.\n",
        "- and this row vector represents a single training example, and assume the corresponding example is of label 2.\n",
        "- we can then do this for 2nd training example.\n",
        "- See, here we have image of 6, 14 X 14 dimentions gray scale image and same for digit 8.\n",
        "- So, concatanate of each images will result in a 196 dimensional feature vector for each of three image.\n",
        "- So we can think of this as our tabular dataset cosists of 3 rows and 196 columns.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=156pNfiX3xO2un70gtDNFv5bVBtQiWc43\" width=\"700\">\n",
        "\n",
        "- here each position represent feature column in the dataset.\n",
        "- In image each pixel value is value and integer in range between `0 to 255`\n",
        "- ðŸŸ¥ MLP don't work very well for more complicated image classification problems.\n",
        "\n",
        "### Drawbacks of MLP\n",
        "\n",
        "- It heavyly realy on certain pixel positions.\n",
        "- see below image, an MLP will not recognize the digit if it is in slightly different position.\n",
        "- Consider the image of 1 that is slightly off centered. and MLP will look at the wrong place in order to classify it. and here in the place where it looks at, there are no pixel values that corresponding to digit 1.\n",
        "- so in this case it will not make prediction as digit 1.\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1vL1LJnn-1iMuAAAJ2VPQF1Ivh9DkfQ4i\" width=\"700\">\n",
        "\n",
        "- Another drawback with MLP is features are considered independently in MLP.\n",
        "- considers each features independent featuers.\n",
        "\n",
        "### CNN\n",
        "\n",
        "- On the other hand CNNs take locality into account.\n",
        "- in given pixel it will also look at surrounding area and makes local region ini image not just individual pixels.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=18xRt5zfoRkVRAKDpt66YTD0Zh8_hfg44\" width=\"700\">\n"
      ],
      "metadata": {
        "id": "mR3pkMLOpIlq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Architecture\n",
        "- Achronyms of CNN: Convnets\n",
        "- CNNs takes locality into account in contraray to MLP.\n",
        "- It helps with chellenging problems for image data.\n",
        "- we want is certain level of invarience that image can still be correctly classified even if its little bit off-center or if image is scaled in terms of bigger or smaller, or image is little bit rotated. Ideally it should also learn to ignore parts of image that are not relevent to the classification task.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1UKQ39Ndg909X0EPFX-ueTE5_mLYqug_A\" width=\"700\">\n",
        "\n",
        "**A Typical CNN Architechture looks like as follows**\n",
        "- We have convolutional layers at center and they are responsible for implicit feature extraction. this is sometimes reffered as deep learning as representation learning problem. Modern deep neural networks are representation learning.\n",
        "- So convolutional layers following the fully connected layers(MLP) that are responsible for classification task.\n",
        "<img src=\"https://drive.google.com/uc?id=1geRU09AhgEXFyBcBvlaeAoWU1KAE_XN2\" width=\"700\">\n",
        "\n",
        "**Different way of drawing CNN architecture**\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1-oDuiiUiw0hGpSOiFdN3-lH4kZJpL7L3\" width=\"700\">\n",
        "\n",
        "- It also has convolution layers then we have fully connected layers MLP on right side.\n",
        "- Highlited part is learnable or trainable layers on the neural network architecture..\n",
        "- the Convolutional layer and fully connected layer have weight parameters and bias units that we can train.\n",
        "- the inbetween part is pooling and they dont have learnable parameters.\n"
      ],
      "metadata": {
        "id": "QV2joVlK8Jpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8JBMc3JNtQfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch CNN Architechture\n",
        "\n",
        "- It consists two block convloutional layers and fully connected layers.\n",
        "- Conv layer contains the conv layers and pooling layers\n",
        "- fully connected layer contains linear layers which takes vector as input. so we need to flatten(convert to vector) the output of conv layer.\n"
      ],
      "metadata": {
        "id": "B39wK4AVtQWY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMKY09H4GkzW"
      },
      "outputs": [],
      "source": [
        "# A typical CNN Architechture of CNN in PyTorch\n",
        "\"\"\"\n",
        "## PyTorch CNN Structure\n",
        "# 1 block is Convolutional block : consists of conv layers and pooling layers\n",
        "# 2 nd block is fully connected layer MLP (classifier part) and this is similar to MLP from previous methodds.\n",
        "#\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyPytorchCNN(torch.nn.Module):\n",
        "  def __init__(self, num_of_classes):\n",
        "    super().__init__()\n",
        "\n",
        "    ## below is the convolutional layers block\n",
        "    # contains conv layers and pooling layers\n",
        "    self.conv_layers = torch.nn.Sequential(\n",
        "        nn.Conv2d(...),\n",
        "        nn.MaxPool2d(...),\n",
        "        nn.Conv2d(...),\n",
        "        nn.MaxPool2d(...),\n",
        "    )\n",
        "\n",
        "    # Fully connected block or classifier block similar to MLP\n",
        "    self.fc_layers = nn.Sequential(\n",
        "        nn.Linear(24 * 16 * 16, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, num_of_classes),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    features = self.conv_layers(x)\n",
        "    features = torch.flatten(features, start_dim=1) # flattening the output of conv layer to prepare to feed to fully connected network.\n",
        "    logits = self.fc_layers(features) # weighted sum from fcl\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Layer\n",
        "- When we apply a convolution to a input image we create a so called **feature map**\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1yahrrKKw7lY_tJCSVfS7otCR02N4btvK\" width=\"700\">\n",
        "\n",
        "### Q. What is a Feature Map and How this process work?\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1yOny1up064ELEFPVSHLrpXbVPv0oOhZZ\" width=\"500\">\n",
        "\n",
        "- here we have grey scale image of hand written digit 5.\n",
        "- And we are going to use **3 X 3 feature detector(kernel, filter)**\n",
        "- We apply these feature detector to these input image\n",
        "- And position by position we slide these feature detector over the image to detect feature map values.\n",
        "- Once we complete the row we move down by 1 position and do the same thing.\n",
        "- and we do this until we have complited all the different values of feature map.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1jzjfQ8rFJOMKAvGbfrW_64qruiOoiooM\" width=\"500\">\n",
        "\n",
        "**What is happening during the operation which is convolution?**\n",
        "- convolution operation is very similar to a dot product. We have different weights, different feature input values, and bias unit.\n",
        "\n",
        "$$ z = b + \\sum_jw_jx_j $$\n",
        "\n",
        "- So in this case if we have 3 X 3 feature detector (filter or kernel) meaning we have 3 X 3 weight matrix. So, if we flatten this weight matrix and the input feature matrix, it will be same thing as dot product between two vectors followed by adding bias unit.\n",
        "- Notice: as we sliding over this image at each position **we have different input feature values**, however we keep reusing the same weight matrix.\n",
        "- So the weights(w's) do not differ.\n",
        "- Since we are using same weights for different positions its called **weight sharing**. Sharing the same set of weights accross different position of the image.\n",
        "- Rational behind the weight sharing is the feature detector that works well in one region may also work well in another region.\n",
        "- and it reduces the complexity by redusing the parameters compared to MLP.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w4ec3jTq0fel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1 input** channel, **1 output** channel"
      ],
      "metadata": {
        "id": "bwu8YwUseaFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Instantiating the convolutional layer with 1 input channel and 1 output channel\n",
        "Key here is that we are using kernel size of 3\n",
        "\"\"\"\n",
        "# kernel size referes to the filter/feature detector size\n",
        "layer = torch.nn.Conv2d(1, 1, kernel_size=3)"
      ],
      "metadata": {
        "id": "TUVlxRp0tLYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer.weight # 3 X 3 weight matrix / filter. we can choose different size of kernels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bqk2E9HeodQ",
        "outputId": "5db35df1-c7e3-4266-8589-616386e6ebb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[ 0.1011, -0.2301, -0.0815],\n",
              "          [ 0.2813, -0.2999,  0.1810],\n",
              "          [ 0.0132, -0.0891,  0.2094]]]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer.bias # bias unit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRhL8BdafNpM",
        "outputId": "55848ef0-22c0-41db-81fc-6ea6cca5d345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-0.1211], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we looked at 1 input channel and 1 output channel. Now let's see convolution with multiple channels"
      ],
      "metadata": {
        "id": "CqFcxVV8gC76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple channel\n",
        "\n",
        "**1 single input** channel, and **multiple** output channel\n",
        "\n",
        "1@ 12X12 - input channel and 3@ 10X10 .`@` represents no of channel\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1_S7eitCFM6cGhUXKq0TotdW1z21HBeLz\" width=\"500\">\n",
        "\n",
        "for 1st output channel\n",
        "$$ z = b + \\sum_jw_jx_j $$\n",
        "\n",
        "\n",
        "for 2nd output channel\n",
        "$$ z = b + \\sum_jw_jx_j $$\n",
        "\n",
        "\n",
        "for 3rd output channel\n",
        "$$ z = b + \\sum_jw_jx_j $$\n",
        "\n",
        "we are using same convolution operation for all the 3 output channels\n",
        "\n",
        "only the difference is for each output channel we use different set of weights. or in other words we are using different feature detectors for different output channel. however, the convolutional operation will be the exactly same.\n",
        "\n",
        "Now let's see working with multiple input channels.\n",
        "\n",
        "**Multiple input** channel and **multiple output** channel\n",
        "- consider input image as color image instead of gray scale. color image has 3 channel RGB red, green, blue.\n",
        "- meaning our input image now has 3 channels.\n",
        "- for simplitsity we only look at 1 output channel.\n",
        "\n",
        "ðŸ“Œ if our input has 3 channel the the kernel/filter will also have 3 channel.\n",
        "- so our kernel now consists of 3 matrixes.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1hwQQPQ2ENvLwBVEliGaH6hNCyJ4-mWZg\" width=\"500\">\n",
        "\n",
        "- So, how do we use 3 channels in kernel?\n",
        "- 1st compute 1 feature map value for each of these channels\n",
        "- 2nd then sum that feature map values will result into output channel.\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1S4h-v_0Sa-qu1rREmaep6SFY5YXmXsu6\" width=\"500\">\n"
      ],
      "metadata": {
        "id": "fCYCOnl9cDwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "k8vT0pqufyug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "While defining the layer we can specify in channels and out channels\n",
        "here we have defined in channels as 3 and out channels as 5\n",
        "kernel is 2 X 2\n",
        "\"\"\"\n",
        "layer = torch.nn.Conv2d(in_channels=3, out_channels=5, kernel_size=2)\n",
        "layer.weight.shape # 5 filter of 3 X 2 X 2, 3 is matrixes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buVyltbjo0TE",
        "outputId": "03f28788-9e8c-4658-91bd-2f26ba76359e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 3, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooling Layer\n",
        "- Cnn layers laerning representations\n",
        "- In convolutional part it **decreases the height and width** of the image, and **increase the no of channels**.\n",
        "- we **use Convolutional layers** to **increas the size of channels**\n",
        "- and to **decrease the height and weidth** we **use pooling layer**\n",
        "\n",
        "### Varients of Pooling\n",
        "1. Max Pooling\n",
        "2. And Average Pooling\n",
        "\n",
        "## 1. Max Pooling\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1XMZlpbVWhqI1G6OQtKSRqUIAHjrnbHtV\" width=\"500\">\n",
        "\n",
        "- Its one of the varient of pooling.\n",
        "- with the given input image, we use **2 X 2** kernel.\n",
        "- What it will do is **takes maximum value and then we keep sliding max pooling kernel over the input** to create the output in the feature map.\n",
        "- To slide this kernel we are using **stride of 2**. meaning we are skipping 2 pixels at a time.\n",
        "\n",
        "## 2. Average Pooling\n",
        "- Similar to max pooling, however we are taking average of the kernel size instead of max value.\n",
        "- Max pooling picks only one value, wheareas average pooling takes average of all the values.\n",
        "- Average pooling retains information about all these 4 pixel values.\n",
        "- Which one works better is another hyperparameter\n",
        "- Often max pooling tends to work better in practice.\n",
        "- Pooling layers dont have any learning parameters. so no weight and bias units to learn.\n",
        "\n",
        "**Why Pooling layer?**\n",
        "\n",
        "- helps in local invarience so it can make network little bit invarient to the exact position of the pixel in image.\n",
        "- downside is some information will be lost. and to avoid this some uses only convolutional layers and skipping the pooling layers.\n",
        "- we can also use conv layer to decrease the height and width by increasing the stride."
      ],
      "metadata": {
        "id": "zVsSNpNApv49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Conv net with pooling layer\n",
        "layers_with_pooling = nn.Sequential(\n",
        "    nn.Conv2d(3, 8, kernel_size=3),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(8, 16, kernel_size=3),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2)\n",
        ")"
      ],
      "metadata": {
        "id": "ztP4Wq_kpQvs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = torch.rand(3, 110, 110)\n",
        "layers_with_pooling(example).shape # with pooling we achived much height and width reductin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8m-nC66ZLk5",
        "outputId": "2fbc3279-647a-45e5-f47e-250bae7ce6f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 26, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conv net without pooling layer.\n",
        "layers_without_pooling = nn.Sequential(\n",
        "    nn.Conv2d(3, 8, kernel_size=3),\n",
        "    # nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(8, 16, kernel_size=3),\n",
        "    # nn.MaxPool2d(kernel_size=2, stride=2)\n",
        ")"
      ],
      "metadata": {
        "id": "R0_B8W0ZZUHH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = torch.rand(3, 110, 110)\n",
        "layers_without_pooling(example).shape # without pooling we dont achive much reduction in height and width."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18un5IRDZnih",
        "outputId": "07651fb7-9cca-48a1-a44f-bfcc93093f57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 106, 106])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conv net without pooling layer.\n",
        "# but by adding stride = 2 to conv layers\n",
        "# we can reduse same width and height.\n",
        "layers_without_pooling1 = nn.Sequential(\n",
        "    nn.Conv2d(3, 8, kernel_size=3, stride=2),\n",
        "    # nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(8, 16, kernel_size=3, stride=2),\n",
        "    # nn.MaxPool2d(kernel_size=2, stride=2)\n",
        ")\n",
        "\n",
        "example = torch.rand(3, 110, 110)\n",
        "layers_without_pooling1(example).shape # without pooling we dont achive much reduction in height and width."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O4MsxglZqgZ",
        "outputId": "6b79971a-4f2d-4d9e-cbc4-bff200bb5262"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 26, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding: to control output size of feature map\n",
        "\n",
        "- It allows us to control the output size more precisesly.\n",
        "\n",
        "**Formula for output width of feature map**\n",
        "- same formula works for height.\n",
        "$$ O = \\frac{W-K+2P}{S}+1$$\n",
        "\n",
        "$$ O = output width \\\\ W = input widht \\\\ K = kernel width \\\\ P = padding \\\\ S = Stride $$\n",
        "\n",
        "- **Floor** operation: if the out put is in point then pytorch will round down to its nearest integer\n",
        "\n",
        "- Padding is the one of the parameter that influence the ouput width.\n",
        "\n",
        "**Example 1**\n"
      ],
      "metadata": {
        "id": "9kJ3wIR4af5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ip width = 100\n",
        "kernel width = 3\n",
        "padding = 0\n",
        "\n",
        "stride = 1\n",
        "\"\"\"\n",
        "op_width = (100-3+0)/1 + 1\n",
        "op_width # 98 pixels\n",
        "\n",
        "\"\"\"\n",
        "With this parameters on inputsize 100\n",
        "it will be output of 98 pixels feature map\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwaux35uaZjm",
        "outputId": "db099e39-57d1-47af-f939-87a088596adf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98.0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate it with the PyTorch Conv layer\n",
        "layer = torch.nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=0)\n",
        "\n",
        "temp_ip = torch.rand(1,100,100)\n",
        "\n",
        "op = layer(temp_ip)\n",
        "op.shape # as we can see its now 98 X 98 pixels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQS48cb9jfV5",
        "outputId": "c1c464a0-09ae-40ce-b380-716380343837"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 98, 98])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 2**"
      ],
      "metadata": {
        "id": "4C5bbM3kmVCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Here we are chaning kernel width to 3 to 5 and increase the size of stride from 1 to 2.\n",
        "and plugging these values into formula we get 48.5.\n",
        "and that is non integer result. let's also compare it with pytorch\n",
        "\"\"\"\n",
        "\n",
        "op_width = (100-5+0)/2 + 1\n",
        "op_width"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkniEXhYkMf9",
        "outputId": "7679bef7-d5bc-4795-fd61-5d3f2d36ffde"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48.5"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = torch.nn.Conv2d(1,1, kernel_size=5, stride=2, padding=0)\n",
        "layer(temp_ip).shape\n",
        "\n",
        "\"\"\"\n",
        "Pytorch is giving 48 but our code gives 48.5\n",
        "\n",
        "It rounds the value to 48.5 ro 48.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cow8-FWhm6lX",
        "outputId": "89305cb3-acd7-419d-bc00-99b2fb5db24f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 48, 48])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working of padding\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1UOGmujUY_KjO1iW-ZBaBbSJzobZifWjA\" width=\"500\">\n",
        "\n",
        "- padding =  1 meaning adding row of 0's to top and bottom and column of 0's to right and left of input image.\n",
        "\n",
        "\n",
        "- assume kernel_size= 3 X 3 and we are sliding it we will get 3 pixels.\n",
        "<img src=\"https://drive.google.com/uc?id=1yky4oHkHw3xEQSxfNentMlC6kwfhRjv_\" width=\"500\">\n",
        "\n",
        "- and when doing same thing with padding, now we have 5 iterations, so using the padding we get a exact feature map width of original size.\n",
        "<img src=\"https://drive.google.com/uc?id=1_H9XjFSgVq3HWdu9bE7Kr7xoBI6JKtl_\" width=\"500\">\n",
        "\n",
        "> So thats how we can control output size using padding parameter.\n",
        "\n",
        "> without doing math we can pass **padding=\"same\"** to **produce same sized outpu t feature map**.\n"
      ],
      "metadata": {
        "id": "1Hh0XW8boniX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oxSzShK8n9HL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}