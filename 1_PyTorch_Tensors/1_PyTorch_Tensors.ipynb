{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch\n",
        "\n",
        "## What is PyTorch?\n",
        "\n",
        "Its 3 things\n",
        "- Tensor library\n",
        "- Automatic differentiation engine\n",
        "- Deep learning library\n",
        "\n",
        "Its `free` and `open source`\n",
        "\n",
        "### History of PyTorch\n",
        "- `PyTorch` is based on `torch` which is another popular library written in `lua` programming language.\n",
        "- Because of most people love python and don't want to learn lua, PyTorch originated from torch.\n",
        "- making it available in python based on torch7\n",
        "- happend in 2016\n",
        "- Most likely used deep learning library for researchers\n",
        "\n",
        "### Tensors\n",
        "Mathematically: It is generalization of vectors, matrices, etc.\n",
        "\n",
        "Computationally: as a data container for storing multi dimentional arrays\n",
        "\n",
        "### 1. Scalar (Rank - 0 Tensor)\n",
        "- In `python` its number\n",
        "- Can think of it as float\n",
        "```py\n",
        "a = 10.\n",
        "print(a)  //10.\n",
        "```"
      ],
      "metadata": {
        "id": "ltc9bNdSPLZr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0XBbI62O0Zv",
        "outputId": "19c27adf-11e5-40cf-b514-e61677c8014c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.0\n"
          ]
        }
      ],
      "source": [
        "a = 10.\n",
        "print(a) # think of it as a float"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Equivalent in `PyTorch`\n"
      ],
      "metadata": {
        "id": "bUg_519N7nUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor(10.)\n",
        "a # scalar tensor or rank-0 tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JExr0s0J7c8d",
        "outputId": "de5a1a1c-2244-4245-c40e-af19f8a6da91"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10.)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# can use a.shape to check dimentionality or rank of a Tensor\n",
        "a.shape\n",
        "\n",
        "\"\"\"\n",
        "It returns nothing because its Rank-0 tensor.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QDxIw0qH8fdU",
        "outputId": "64d16f0f-decb-4d0d-e1d2-1ac7eeb48698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIt returns nothing because its Rank-0 tensor.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Vectors (Rank-1 Tensor)\n",
        "- In Python, we can think simple list as a vector/ Rank-1 tensor\n"
      ],
      "metadata": {
        "id": "Cesrl2Sk9BZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = [1., 2., 3.]\n",
        "a # Vector: simple list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otM4L5uf8y7Q",
        "outputId": "92334a00-6a79-4546-a43b-1f2a83c2f930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0, 2.0, 3.0]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In PyTorch, its same as before but wrapping the list to the `torch.tensor`\n"
      ],
      "metadata": {
        "id": "7xr_yscm-Gtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1., 2., 3.])\n",
        "a # Vactor/Rank-1 Tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeHfIXIT9tEv",
        "outputId": "15b6b4d0-41ca-4645-8986-094ec75a7af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape # will return 3\n",
        "# because its 3 element tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ5ETZCl-ZL-",
        "outputId": "cd905c71-469a-48c4-c954-94e29dd2c99a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Matrices (Rank-2 Tensor)\n",
        "- Here we use list of lists.\n",
        "- This list has two sub list, and each of the sub list represents the row, So this will result in a matrix consisting of 2 rows and 3 columns.\n",
        "- We can think of the rows as a `training example`, and columns represnts the `features` of the dataset"
      ],
      "metadata": {
        "id": "DUEvN1ja-zEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1., 2., 3.],\n",
        "                  [4., 5., 6.]])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9dCyKPf-qdV",
        "outputId": "f5d159ca-347a-4503-eace-83a3c16cf6fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3.],\n",
              "        [4., 5., 6.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6RMiZNv_Q7N",
        "outputId": "42507713-c799-4144-8b27-d6b345100f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Considering realworld dataset\n",
        "- We can think image as a matrix\n",
        "\n",
        "<img src=\"https://images.pexels.com/photos/539694/pexels-photo-539694.jpeg\" width=\"400\" >\n",
        "\n",
        "- Where the rows and the columns represents the pixels of the image.\n",
        "- Here the image refers to 1 training example\n",
        "\n",
        "**Look at RGB image**\n",
        "\n",
        "Red, green, blue, 3 different color channels\n",
        "\n",
        "- So, in above image we have 3 color channels rgb.\n",
        "- we can think of this as a `stack of matrices`.\n",
        "- each layer/color channel represents the matrix.\n",
        "- as we already know scalar(rank-0 tensor), vectors(rank-1 tensor), and matrices(rank-2 tensors).\n",
        "\n",
        "### 4. 3D-tensors\n",
        "- So we this 3 dimentional data we call it **3D-tensor**\n",
        "- We can think stack of matrices as a 3D tensors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aHwmLEHpVYI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[[1., 2., 3.],[2., 3., 4.]],\n",
        "                  [[4., 5., 6.], [7., 8., 9.]]])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcRhdyW7_SfU",
        "outputId": "e2bbf994-bfc3-492f-ed8c-29155319f83c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 2., 3.],\n",
              "         [2., 3., 4.]],\n",
              "\n",
              "        [[4., 5., 6.],\n",
              "         [7., 8., 9.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "So, when we use a.shape it returns 3 numbers\n",
        "- each number represents one dimention or one rank,\n",
        "- and then numbers represents the values in this dimention\n",
        "\"\"\"\n",
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfK5GNVrbALh",
        "outputId": "2c010d1f-2b81-4b67-d744-8c7c268aba60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 4D(rank-4)tensor\n",
        "\n",
        "Going 1 step further we can also have a stack of multiple color images.\n",
        "\n",
        "this would add another dimention.\n",
        "\n",
        "And in this case we have 4 dimentional tensor or rank-4 tensor"
      ],
      "metadata": {
        "id": "GatqeFvcd5eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = torch.stack((a, a))\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g7TqJdcbQ72",
        "outputId": "042b0bf1-96e4-48d4-bd00-a85d1a885995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1., 2., 3.],\n",
              "          [2., 3., 4.]],\n",
              "\n",
              "         [[4., 5., 6.],\n",
              "          [7., 8., 9.]]],\n",
              "\n",
              "\n",
              "        [[[1., 2., 3.],\n",
              "          [2., 3., 4.]],\n",
              "\n",
              "         [[4., 5., 6.],\n",
              "          [7., 8., 9.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xQLLJwngIMf",
        "outputId": "ef2f22f0-47a8-4537-8944-db91b12388dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it looks similar like numpy's array, now let's see how tensor is different than a numpy array.\n"
      ],
      "metadata": {
        "id": "8Dn6kBzQgWDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Tensor library with Array library\n",
        "\n",
        "- they are actually the same thing.\n",
        "- tensor library = array library\n",
        "- torch.tensor ~= numpy.array: torch.tensor is almost identical to numpy.array\n",
        "\n",
        "Difference\n",
        "\n",
        "|torch.tensor|\n",
        "|:-|\n",
        "|+ supports GPU computation|\n",
        "|+ Automatic differentiation support, very useful when training neural nets|\n",
        "\n",
        "### How tensors and arrays differe from regular python lists?\n",
        "\n",
        "**Python Lists**\n",
        "+ **Pros:** Can store heterogeous types (mix str, float, etc). you can store float, strings, and other objects mixed in a list.\n",
        "+ **Pros:** In python list we can easily remove or add items using `.append` or `.pop`\n",
        "+ **Cons:** while lists are easy to use and flexible, lists are very slow when it comes to numerical computation(that is the main motivation behind tensors)\n",
        "\n",
        "**Tensors**\n",
        "- Limitations of using tensors though is that all elements in tensors have to be the same type(eg. float, integer)\n",
        "- In contrast to lists, tensors also have a fixed size, so we can't easily add or remove If we want to have a larger tensor, we have to create new empty tensor with a larger size and copy over the all the elements and add the new elements to it\n",
        "\n",
        "this sounds like tensors are bad, However tensors have certain advantages over the lists which are extreamly useful for deep learning, which is heavily based on numerical computations.\n",
        "\n",
        "- tensors support wide variety of different computations.\n",
        "- numerical computations are fast"
      ],
      "metadata": {
        "id": "y0UMOFqZ49uJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Tensors In PyTorch\n",
        "\n",
        "1. `torch.tensor()`: Creating Tensors\n",
        "    - its most fundamental function. bcs that's how we create a tensors in PyTorch"
      ],
      "metadata": {
        "id": "e7XJ0SC6I9zK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1., 2., 3.])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54hdqrdagJYm",
        "outputId": "38635206-9ef1-4b88-9f37-208a8015d615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. `.shape` Checking Shape of Tensors\n",
        "    - tensor.shape to check the shape of the tensor.\n",
        "    - using `.shape` attribute we can check the no of elements in the tensor.\n",
        "    - in 2D tensor 1st no referes no of rows in tensor, and 2nd no referes no of columns in tensor.\n",
        "    - to check rank of the tensor count the no of no that are returned by `.shape`"
      ],
      "metadata": {
        "id": "4RiRWL7pJl_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1., 2., 3.], [3., 4., 5.]])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBrXSvpeJkSS",
        "outputId": "35909e79-2dcb-424c-bb82-bd1a337461fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3.],\n",
              "        [3., 4., 5.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape # will get [2,3] tow numbers meaning its 2D tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lIrwXutLCGB",
        "outputId": "b920f1bb-8d7a-4377-8f13-96098b05811f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. `.ndim`: Checking the Rank/ Number of Dimentions\n",
        "    - use .ndim to check rank or dimention of the tensor"
      ],
      "metadata": {
        "id": "0F85ZAzXLNu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[[1., 2., 3.], [4., 5., 6.]], [[3., 4., 5.], [6., 7., 8.]]])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwwDi5AyLK0w",
        "outputId": "97ae6924-56ed-46ec-87da-ab863ebd6b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 2., 3.],\n",
              "         [4., 5., 6.]],\n",
              "\n",
              "        [[3., 4., 5.],\n",
              "         [6., 7., 8.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.ndim # will get dimention of the tensor, in our case its 3D tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzMO-VUmL0_d",
        "outputId": "4899bfdc-f3ac-4d5d-b859-8dd19b92ede0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. `.dtype`: Checking the Data type of Tensor\n",
        "    - as tensor can only store same type of data.\n",
        "    - we can see the datatype of the tensor.\n",
        "    - below it returns torch.float32, meaning 32 bit precision\n",
        "    - that's prefered precision in deep learning bcs of efficiency reasons"
      ],
      "metadata": {
        "id": "NlZoYESVMFVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1., 2., 3.], [5., 6., 7.]])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SClvJKvNL1yv",
        "outputId": "ce58612d-42f2-43ef-ba09-ff2048f46641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3.],\n",
              "        [5., 6., 7.]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.dtype # will get data type of tensor that is float32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0vK_tEkMj2v",
        "outputId": "03e86eb6-52b5-41ca-b11c-dc261203787e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1,2,3],[4,5,6]])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf9Psh6IMlUH",
        "outputId": "a8af3349-a11c-45c8-a0b9-7a68f12c8117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.dtype # int datatype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK5xiyrTNWkR",
        "outputId": "8fb42e38-e74f-4acf-c3a1-76378e305ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. `torch.from_numpy(np_array)` Creating Tensor from Numpy Array\n",
        "    - torch has the `from_numpy()` function which lets us convert Numpy array directly into Tensor.\n",
        "    - can also call `.tensor` on numpy array, but that would create a copy in memory.\n",
        "    - using from_numpy() function it will use the same memory as the numpy array.\n",
        "    - since python uses 64bit precision bydefault, the converted tensor from numpy will be the same 64bit.\n"
      ],
      "metadata": {
        "id": "TM6UDSv7itqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "gQ-vp6CLNYfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_array = np.array([1., 2., 3.]) # creating numpy array\n",
        "print(f\"Numpy array: {np_array}\")\n",
        "m2 = torch.from_numpy(np_array) # creating tensor from numpy array, its dtype will be same as numpy's that is 64bit,\n",
        "# its 64 because its default type in numpy\n",
        "m2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojOeSKupsC4W",
        "outputId": "f4811afe-a4d8-4d94-8c6e-b128614b8150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy array: [1. 2. 3.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. `tensor_obj.to(new_dtype)` Change the dtype"
      ],
      "metadata": {
        "id": "CgV57S2Ws1Oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m2 # currently its float64 dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfrOD4oTsbyD",
        "outputId": "555aeab1-f567-45ee-c852-cdfb00d39f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m2 = m2.to(torch.float32) # changing 64bit datatype to 32\n",
        "m2.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyODemU2s7oI",
        "outputId": "365d9485-c143-4d38-b194-6caf4eefbb74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. `.device` Checking the device Type\n",
        "    - tensors also have a `.device` attribute, that show us where on our computer the tensor is located.\n",
        "    - So usually it will return CPU which means tensor is on CPU's memory.\n",
        "    - Later will see how to transfer tensors to the GPU which can be very useful for deep learnig and accelerating training."
      ],
      "metadata": {
        "id": "lc5vsaADxaFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m2.device # currently its on cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFQKLt5NwPoS",
        "outputId": "a7edc9f3-dee9-4684-d617-d021a751733a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Changing Shape of a Tensor\n",
        "    - Using `.view()` function we can change the shape of the tensor\n",
        "    - `.view(-1,2)` here `-1` means it will automatically decide the no of rows/ dimention. here we're saying that we want no of columns 2, and because there's only one way it can have 2 columns it will put 3 rows automatically for us.\n",
        "    - `.view(2, -1)` and if we use `-1` in column placeholder, that means it will automatically determine that dimention"
      ],
      "metadata": {
        "id": "2cy2M0q2yppA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.shape)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VENnmoXnyjCY",
        "outputId": "796a2556-2841-4cd4-db6d-69f6de1b0f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.view(3,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0GzDL3fy75U",
        "outputId": "4a62407e-94b8-4ae9-aa8a-1d7c715eb43c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4],\n",
              "        [5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.view(-1,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71Qr_WvezIig",
        "outputId": "6edf21c9-75ac-4694-eec8-8f0252cb6042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Transposing a Matrix\n",
        "    - There's a concept of transposing a tensor like matrix transpose.\n",
        "    - Transposing a matrix meaning flipping it along its diagonal. like row values becomes column and column becomes rows."
      ],
      "metadata": {
        "id": "KYzj6uLa6ZtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n",
        "m # before transposing a matrix its shape is 2,3. 2 rows and 3 columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LjVrMtszsXm",
        "outputId": "7c820c55-4a81-4eba-9ce3-f94663b7f5b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3.],\n",
              "        [4., 5., 6.]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.T # applying transpose operation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emOoCXfK6e6G",
        "outputId": "3c78fc10-5d26-48c8-8408-ccb50e2e529e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 4.],\n",
              "        [2., 5.],\n",
              "        [3., 6.]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Multiplying Matrices\n",
        "    - This is something that we do a lot in deeplearning.\n",
        "    - Can use concepts from linear algebra to make code more efficient and faster"
      ],
      "metadata": {
        "id": "-qzAfMj6O2SM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improving Code with Linear Algebra\n",
        "\n",
        "- From for loop to dot product\n",
        "- As we already know basics of tensors, let's see how to use concepts from linear algebra to encode certain things like the weighted sum more efficiently.\n",
        "\n",
        "**Weighted Sum**\n",
        "\n",
        "- its computed by multiplying inputs with the weights\n",
        "\n",
        "`TODO: Image of neuron`\n",
        "\n",
        "$$ b + x_1w_1 + x_2w_2 $$\n",
        "\n",
        "- we do this computation for each individual training example.\n",
        "\n",
        "1 training example with 2 features values\n",
        "\n",
        "$$ z = b + x_1 * w_1 + x_2 * w_2 $$\n",
        "\n",
        "General form\n",
        "\n",
        "$$ z = b + ‚àë_{j=1}^m x_jw_j $$\n",
        "\n",
        "- we can express weighted sum is dot product between `x` and `w` two vectors. if `X`and `W` are vectors.\n",
        "\n",
        "$$  = b + XW $$\n",
        "\n",
        "- can use concept from linear algebra called dot product to compute weighted sum\n"
      ],
      "metadata": {
        "id": "u2a7ZhAVsuzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dot product or multiplication of vectors in plain Python\n",
        "b = 0\n",
        "x = [1.2, 2.2]\n",
        "w = [3.3, 4.3]\n",
        "\n",
        "output = b\n",
        "for xi, wi in zip(x,w):\n",
        "  output += xi * wi\n",
        "\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQOM_D7J6noc",
        "outputId": "9e0325ac-8b4b-442d-e77f-238718ac423b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.42"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dot product between 2 vectors\n",
        "\n",
        "**Another way to look at it is for single training example**\n",
        "\n",
        "$$ z = b + X^TW $$\n",
        "\n",
        "Let's see step by step\n",
        "\n",
        "`X` is a vector from 1 to m. and `W` is the vector from 1 to m.\n",
        "\n",
        "$$ X = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ .\\\\.\\\\.\\\\x_m\n",
        "\\end{bmatrix} W = \\begin{bmatrix}w_1 \\\\ w_2 \\\\ .\\\\.\\\\.\\\\w_m\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "- now adding a transpose operation here. It will become handy during matrix multiplication.\n",
        "- Transpose operation is transposing the column vector `X` into row vector `X^T` x transpose.\n",
        "\n",
        "### `.dot()` to compute the dot product of two vectors\n",
        "\n",
        "- It's much simpler and compact than venila python using for loop.\n",
        "- Biggest advantage of using `.dot()` dot product is its **much much faster**.\n",
        "- we can use `.dot()` dot product to improve code efficiency.\n",
        "- question is how much faster? let's do benchmark"
      ],
      "metadata": {
        "id": "2i2Sl5uUe_Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dot product of two vectors using .dot using PyTorch\n",
        "b = torch.tensor(0.)\n",
        "x = torch.tensor([1.2, 2.2])\n",
        "w = torch.tensor([3.3, 4.3])\n",
        "\n",
        "x.dot(w) + b  # much simpler than the plain python. It's more compact than venila python using for loop\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQlM27AIeVuP",
        "outputId": "43777706-6a67-40f0-ef5a-3c9746640a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(13.4200)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FXRWz7qkvek_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarking Venila Python List and PyTorch dot product\n"
      ],
      "metadata": {
        "id": "rRTGzIGj3vb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plain_python(x, w, b):\n",
        "  out = b\n",
        "  for xi, wi in zip(x,w):\n",
        "    out += xi * wi\n",
        "  return out\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(123)\n",
        "\n",
        "b = 0\n",
        "x = [random.random() for _ in range(1000)]\n",
        "w = [random.random() for _ in range(1000)]\n",
        "\n",
        "\n",
        "%timeit plain_python(x, w, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzdF6PAVw2Ds",
        "outputId": "b7035bde-18d4-4a75-8ee2-ff2a7af2b6f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84.7 ¬µs ¬± 1.55 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In PyTorch\n",
        "def pytorch_dot(x, w, b):\n",
        "  return x.dot(w) + b # dot product using pytorch\n",
        "\n",
        "# First convert python list to tensor\n",
        "b = torch.tensor(b)\n",
        "x = torch.tensor(x)\n",
        "w = torch.tensor(w)\n",
        "\n",
        "%timeit pytorch_dot(x, w, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAGr-it4x7JO",
        "outputId": "2c5c26a3-c528-4cb3-ebd5-2ddad7e65f5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.7 ¬µs ¬± 2.08 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing runtime of plain python with pytorch tensor there is 25X speedup! in pytorch, which is huge.\n",
        "\n",
        "so that's a good reason to replace python for loop with pytorch dot product.\n",
        "\n",
        "So far it was about computation/weighted sum of single vector/training example. However in practice in deep learning we usually work with large dataset. and for that we use matrix multiplication."
      ],
      "metadata": {
        "id": "b_Nqmi8V4C0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dealing with multiple training examples via matrix multiplication\n",
        "\n",
        "## Matrix multiplication of matrix and a vector.\n",
        "It can be seen as multiple input examples for single neuron/node.\n",
        "\n",
        "- Let's see applying dot product to multiple training example using matrix multiplication\n",
        "- previously it was only 1 training example for computing weighted sum.\n",
        "- extending that with multiple training examples\n",
        "\n",
        "\n",
        "$$ z^{[i]} = b + ‚àë_{j=1}^m x_jw_j^{[i]} $$\n",
        "`i` referes to the training example index.\n",
        "\n",
        "- one way to look at it is as computing each weighted sum individually for each training example\n",
        "- computing weighted sum `n` times if dataset contains `n` training examples\n",
        "\n",
        "$$ z^{[1]} = b + ‚àë_{j=1}^m x_jw_j^{[1]} $$\n",
        "$$ z^{[2]} = b + ‚àë_{j=1}^m x_jw_j^{[2]} $$\n",
        "$$...$$\n",
        "$$ z^{[n]} = b + ‚àë_{j=1}^m x_jw_j^{[n]} $$\n",
        "\n",
        "**Look at more compact form**\n",
        "$$ z^{[1]} = b + X^{[1]}W $$\n",
        "$$ z^{[2]} = b + X^{[2]}W $$\n",
        "$$...$$\n",
        "$$ z^{[n]} = b + X^{[n]}W $$\n",
        "\n",
        "- zoom into the `X`, each `X` represents the vector\n",
        "- this is the feature vector corresponding to each training example.\n",
        "- we can take these `vectors` and represent as a `matix`.\n",
        "\n",
        "$$ X = \\begin{bmatrix}x^{[1]} \\\\ x^{[2]} \\\\ .\\\\.\\\\.\\\\x^{[n]}\n",
        "\\end{bmatrix} $$\n",
        "\n",
        "- each row here is a single training example.\n",
        "- expanding it each column represents one feature.\n",
        "\n",
        "$$ X = \\begin{bmatrix}x^{[1]} \\\\ x^{[2]} \\\\ .\\\\.\\\\.\\\\x^{[n]}\n",
        "\\end{bmatrix} = \\begin{bmatrix}x_1^{[1]} x_2^{[1]}...x_m^{[1]} \\\\ x_1^{[2]} x_2^{[2]}...x_m^{[2]} \\\\ .\\\\.\\\\.\\\\ x_1^{[n]} x_2^{[n]}...x_m^{[n]}\n",
        "\\end{bmatrix} $$\n",
        "\n",
        "- here **each feature column still corresponds to single weight**.\n",
        "\n",
        "$$ X = \\begin{bmatrix}x_1^{[1]} x_2^{[1]}...x_m^{[1]} \\\\ x_1^{[2]} x_2^{[2]}...x_m^{[2]} \\\\ .\\\\.\\\\.\\\\ x_1^{[n]} x_2^{[n]}...x_m^{[n]}\n",
        "\\end{bmatrix}  W = \\begin{bmatrix}w^{[1]} \\\\ w^{[2]} \\\\ .\\\\.\\\\.\\\\w^{[m]}\n",
        "\\end{bmatrix}  $$\n",
        "\n",
        "### My understanding\n",
        "- each feature meaning no of inputs to the network.\n",
        "- and its same size for weight parameter vector. because no of weights will be same as no of inputs for the single neuron/perceptron.\n",
        "- as per above explaination `X` matrix the no of rows are training examples and it has total `m` inputs from single example. and corresponding weights of size `m` which is same as no of inputs.\n",
        "- üìçNOTE: here its `W` weight vector that means its just single neuron or node.\n",
        "\n",
        "\n",
        "---\n",
        "- Here we still have a weight vector and not a weight matrix, bacause we use the same weight for each training example.\n",
        "- in this case `w1` will be used for feature column `x1` the whole 1st column.\n",
        "- `w2` would be used for feature column 2 `x2`, and so for.\n",
        "- so for dataset with `m` features we still have weight vector consisting of `m` `weights`.\n",
        "- üí°Note: ensure that weight matrix X hase the same no of columns as the weight vector rows. we can then compute matrix multiplication via dot product.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uflNhx1pxxRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarking\n",
        "\n"
      ],
      "metadata": {
        "id": "6ub2IEKvLo_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python simple for loop\n",
        "#----------\n",
        "\n",
        "b = 0.\n",
        "X = [[1.2, 2.2],\n",
        "     [4.4, 5.5]] # 2 inputs 2d vector\n",
        "w = [3.3, 4.3] # weight vector\n",
        "\n",
        "outputs = []\n",
        "for x in X: # computing matmul for each row/inputs with weight vector\n",
        "  output = b\n",
        "  for xi, wi in zip(x, w):\n",
        "    output += xi*wi\n",
        "  outputs.append(output)\n",
        "\n",
        "outputs # final output"
      ],
      "metadata": {
        "id": "Yf89T72nyg4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0af680c0-09e6-4ef3-aef0-0f4dd03b22c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[13.42, 38.17]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch matmul\n",
        "#-----------------\n",
        "\n",
        "# Converting python lists and values to tensor\n",
        "b = torch.tensor(b)\n",
        "X = torch.tensor(X)\n",
        "w = torch.tensor(w)\n",
        "\n",
        "# Applying matrix multiplication\n",
        "X.matmul(w) + b # will return tensor result of matrix multiplication"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J_2EQN2Mags",
        "outputId": "07c9ba50-127e-48b5-b930-dcd92a3c9038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([13.4200, 38.1700])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As results are exacly same\n",
        "\n",
        "**Benchmarking with simple Python**"
      ],
      "metadata": {
        "id": "C7dyzdiEOio3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123) # initializing the specific random value\n",
        "\n",
        "# initializing input matrix X and weight vector w\n",
        "b = 0.\n",
        "X = [[random.random() for _ in range(1000)] for _ in range(500)] # 500 rows and 1000 columns\n",
        "w = [random.random() for _ in range(1000)]\n",
        "\n",
        "# definging function that does weighted sum of input matrix and weight vector.\n",
        "def plain_python(X, w, b):\n",
        "  outputs = []\n",
        "  for x in X:\n",
        "    output = b\n",
        "    for xi, wi in zip(x, w):\n",
        "      output += xi * wi\n",
        "    outputs.append(output)\n",
        "\n",
        "  return outputs\n",
        "\n",
        "%timeit plain_python(X, w, b) # will give us time took to execute the matmul in plain python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy87rYkSNILe",
        "outputId": "486ce5a9-1f3c-4c0f-8be6-a9ad7049229b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43.5 ms ¬± 1.18 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benchmarking with PyTorch matmul**\n"
      ],
      "metadata": {
        "id": "mI4xsAjzQl8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting\n",
        "b = torch.tensor(b)\n",
        "X = torch.tensor(X)\n",
        "w = torch.tensor(w)\n",
        "\n",
        "def pytorch_matmul(X, w, b):\n",
        "  return X.matmul(w) + b\n",
        "\n",
        "%timeit pytorch_matmul(X, w, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU2ch2vRQJsS",
        "outputId": "a2659955-9634-4d81-cb4d-d10bc358309b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93.8 ¬µs ¬± 10.2 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üü© As we can see PyTorch is approx. 1000X faster in speed which is hugeüî•\n",
        "\n",
        "üü© Its now clear why we prefer to use PyTorch over plain Python when implementing neural nets.\n",
        "\n",
        "> ‚úçÔ∏è So that was matrix multiplication between matrix and a vector."
      ],
      "metadata": {
        "id": "fLpiJ2hxRlEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiply two matrix\n",
        "---\n",
        "- It will be more relevent in multilayer perceptron.\n",
        "- Can think it as weight matrix, and its rows, each row contains weights of the each neuron in the layer.\n",
        "\n",
        "\n",
        "$$ X = \\begin{bmatrix}x_1^{[1]} x_2^{[1]}...x_m^{[1]} \\\\ x_1^{[2]} x_2^{[2]}...x_m^{[2]} \\\\ .\\\\.\\\\.\\\\ x_1^{[n]} x_2^{[n]}...x_m^{[n]}\n",
        "\\end{bmatrix}  W = \\begin{bmatrix}w_1^{(1)} w_2^{(1)},...,w_m^{(1)} \\\\ w_1^{(2)} w_2^{(2)},...,w_m^{(2)}  \\\\ .\\\\.\\\\.\\\\ w_1^{(h)} w_2^{(h)},...,w_m^{(h)}\n",
        "\\end{bmatrix}  $$\n",
        "\n",
        "`n` training examples.\n",
        "\n",
        "`h` weight vectors. can think each row as each node's weight coresponding to input row/vector.\n",
        "\n",
        "- In training deep neural nets we often use matrix multiplication because we have a weight matrix now.\n",
        "\n",
        "- So input is the same `X` as the training dataset.\n",
        "- However, now we have a weight matrix `W`. And in weight matrix, the rows represents the different features.\n",
        "- in addition we have columns now, and each columns produces different output.\n",
        "- can think of it as perceptron's weighted sum but with multiple outputs. each column refers to different output.\n",
        "- in essence we can think of these weight matrix as a weight matrix consisting of `h` weight vectors, each weight vector corresponds to 1 output.\n",
        "- In order to multiply input matrix `X` with weight matrix `W`, we have to **make sure that the dimentions match** and **we are computing the right thing**\n",
        "- üí° so we want to compute dot product of each training example with each weight vector. So we do a transpose of the weight matrix.\n",
        "\n",
        "$$ X = \\begin{bmatrix}x_1^{[1]} x_2^{[1]}...x_m^{[1]} \\\\ x_1^{[2]} x_2^{[2]}...x_m^{[2]} \\\\ .\\\\.\\\\.\\\\ x_1^{[n]} x_2^{[n]}...x_m^{[n]}\n",
        "\\end{bmatrix}  W^{T} = \\begin{bmatrix}w_1^{(1)} w_1^{(2)},...,w_1^{(h)} \\\\ w_2^{(1)} w_2^{(2)},...,w_2^{(h)}  \\\\ .\\\\.\\\\.\\\\ w_m^{(1)} w_m^{(2)},...,w_m^{(h)}\n",
        "\\end{bmatrix}  $$\n",
        "\n",
        "- So, now each column of right hand side of weight matrix that is `W transpose` corresponds one weight vector. Then just like before we can compute the dot product between each row in dataset `X` and weight matrix `W`.\n",
        "- to obtain first output value do dot product between `1st row` in input matrix and first column of weight matrix. and computing for each row in input we will get 1st column in result.\n",
        "- doing this till all the column of weight matrix we will get full result.\n"
      ],
      "metadata": {
        "id": "20YH8FX-retb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(100, 10)\n",
        "W = torch.rand(50, 10)\n",
        "\n",
        "R = torch.matmul(X, W.T) # two matrix multiplication using pytorch\n"
      ],
      "metadata": {
        "id": "ECNAj7eERb6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmRs9FZ6Qtxm",
        "outputId": "f1ac20f0-63fe-4f1a-dad4-001d526ee36b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WROqC37Q1AW",
        "outputId": "737a017c-c160-4156-a9ce-498fd2c3ce3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "R.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QypUIZfsO0Ur",
        "outputId": "13c9c68f-15b0-4cc9-858d-23f43b8343f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarking\n"
      ],
      "metadata": {
        "id": "p_iyAGm9O7rS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pytorch_matmul(X, W):\n",
        "  return torch.matmul(X, W.T)\n",
        "\n",
        "%timeit pytorch_matmul(X,W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk2N7bt-O3Sc",
        "outputId": "b6e4a566-b5bd-4fdd-cd47-27e67d355b5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.4 ¬µs ¬± 140 ns per loop (mean ¬± std. dev. of 7 runs, 100000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Broadcasting\n",
        "\n",
        "let's see how we can work with different shapes of tensor.\n",
        "- previously we saw how we can use concepts of linear algebra to make our code more efficient.\n",
        "- Now, `Broadcasting` how to do computations with the unequal tensor shapes to save us some typing work\n",
        "\n",
        "eg.\n",
        "\n",
        "v1 = [1.1, 2.1, 3.1, 4.1] n = 5.6\n",
        "\n",
        "want to do\n",
        "`[1.1, 2.1, 3.1, 4.1] + 5.6`\n",
        "\n",
        "- here we have vector `v1` and we want to add `n`'s value to the vector.\n",
        "- we just can't add a single number to vector.\n",
        "- However, tensor and array library have a concept called `broadcasting` where they create dimentions implicitly.\n",
        "\n",
        "PyTorch will infer that we actually want to add vector consisting of 5.6 to the vector `v1` that we use to add. And this concept is called `Broadcasting`\n",
        "\n",
        "like this `[1.1, 2.1, 3.1, 4.1] + [5.6, 5.6, 5.6, 5.6]` its called Broadcasting.\n",
        "\n",
        "So when we add number to vector, output will be a vector where the same number is added to each element in the input vector."
      ],
      "metadata": {
        "id": "XP0uj2n2R_TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1.1, 2.1, 3.1, 4.1])\n",
        "b = torch.tensor([5.6])\n",
        "\n",
        "a + b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc1T6zYQQmLj",
        "outputId": "2fe8dc4a-338b-49fc-b847-0e8b8096ba2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6.7000, 7.7000, 8.7000, 9.7000])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Similarly we can add `vector` to a `matrix`.**"
      ],
      "metadata": {
        "id": "GsUzW0baanL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.tensor([[1.1, 2.1, 3.1, 4.1],\n",
        "                 [1.2, 2.2, 3.2, 4.2]])\n",
        "\n",
        "b = torch.tensor([5.4, 5.5, 5.6, 5.7])\n",
        "\n",
        "A + b # addition of matrix A and vector b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43k0J1r5ZdFa",
        "outputId": "d8a72dea-ab3b-4894-876f-b45bd16638e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6.5000, 7.6000, 8.7000, 9.8000],\n",
              "        [6.6000, 7.7000, 8.8000, 9.9000]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean operation\n"
      ],
      "metadata": {
        "id": "QuJkOaEtmO7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1., 2., 3.], [3., 2., 5.]])\n",
        "a"
      ],
      "metadata": {
        "id": "FeEdVXLmet-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74a709ac-8e26-4f13-8c39-f0824251f6f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3.],\n",
              "        [3., 2., 5.]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.mean(dim=0) # or torch.mean(a, dim=0).  dim = 0 meaning it will work on column values to mean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMc11jMQmkyJ",
        "outputId": "c5673776-7920-4fbb-85ea-2e14e556f59e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 2., 4.])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.mean(dim=1) # or torch.mean(a, dim=1) dim = 1 meaning it will work on row values, mean of row values."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2BG_AWomoqx",
        "outputId": "65ea6cf0-fc3a-4e68-fd2d-24a54a353084"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.0000, 3.3333])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zUripA0xn9Lw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}