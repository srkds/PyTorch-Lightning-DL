{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch\n",
        "\n",
        "## What is PyTorch?\n",
        "\n",
        "Its 3 things\n",
        "- Tensor library\n",
        "- Automatic differentiation engine\n",
        "- Deep learning library\n",
        "\n",
        "Its `free` and `open source`\n",
        "\n",
        "### History of PyTorch\n",
        "- `PyTorch` is based on `torch` which is another popular library written in `lua` programming language.\n",
        "- Because of most people love python and don't want to learn lua, PyTorch originated from torch.\n",
        "- making it available in python based on torch7\n",
        "- happend in 2016\n",
        "- Most likely used deep learning library for researchers\n",
        "\n",
        "### Tensors\n",
        "Mathematically: It is generalization of vectors, matrices, etc.\n",
        "\n",
        "Computationally: as a data container for storing multi dimentional arrays\n",
        "\n",
        "### 1. Scalar (Rank - 0 Tensor)\n",
        "- In `python` its number\n",
        "- Can think of it as float\n",
        "```py\n",
        "a = 10.\n",
        "print(a)  //10.\n",
        "```"
      ],
      "metadata": {
        "id": "ltc9bNdSPLZr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0XBbI62O0Zv",
        "outputId": "4fa4a152-4c15-42cc-91e0-9f1e62568fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.0\n"
          ]
        }
      ],
      "source": [
        "a = 10.\n",
        "print(a) # think of it as a float"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Equivalent in `PyTorch`\n"
      ],
      "metadata": {
        "id": "bUg_519N7nUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor(10.)\n",
        "a # scalar tensor or rank-0 tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JExr0s0J7c8d",
        "outputId": "0a93fa87-284c-4032-f990-6916856b23ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10.)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# can use a.shape to check dimentionality or rank of a Tensor\n",
        "a.shape\n",
        "\n",
        "\"\"\"\n",
        "It returns nothing because its Rank-0 tensor.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QDxIw0qH8fdU",
        "outputId": "18629ee5-582c-4a7f-a5eb-662997692e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIt returns nothing because its Rank-0 tensor.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Vectors (Rank-1 Tensor)\n",
        "- In Python, we can think simple list as a vector/ Rank-1 tensor\n"
      ],
      "metadata": {
        "id": "Cesrl2Sk9BZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = [1., 2., 3.]\n",
        "a # Vector: simple list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otM4L5uf8y7Q",
        "outputId": "96b7ff06-efef-473f-9663-f9fdf1e58f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0, 2.0, 3.0]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In PyTorch, its same as before but wrapping the list to the `torch.tensor`\n"
      ],
      "metadata": {
        "id": "7xr_yscm-Gtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1., 2., 3.])\n",
        "a # Vactor/Rank-1 Tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeHfIXIT9tEv",
        "outputId": "7fc4c1a4-5713-4873-dd81-57a4dfd186e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape # will return 3\n",
        "# because its 3 element tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ5ETZCl-ZL-",
        "outputId": "c54039ec-5ad4-4b3f-d6cb-f9d3cc9991d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Matrices (Rank-2 Tensor)\n",
        "- Here we use list of lists.\n",
        "- This list has two sub list, and each of the sub list represents the row, So this will result in a matrix consisting of 2 rows and 3 columns.\n",
        "- We can think of the rows as a `training example`, and columns represnts the `features` of the dataset"
      ],
      "metadata": {
        "id": "DUEvN1ja-zEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1., 2., 3.],\n",
        "                  [4., 5., 6.]])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9dCyKPf-qdV",
        "outputId": "5c339fa7-ea5f-4e30-c35b-bc2fed16a2eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3.],\n",
              "        [4., 5., 6.]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6RMiZNv_Q7N",
        "outputId": "40f51a93-0a93-4f81-bc01-084bd9cbb751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Considering realworld dataset\n",
        "- We can think image as a matrix\n",
        "\n",
        "<img src=\"https://images.pexels.com/photos/539694/pexels-photo-539694.jpeg\" width=\"400\" >\n",
        "\n",
        "- Where the rows and the columns represents the pixels of the image.\n",
        "- Here the image refers to 1 training example\n",
        "\n",
        "**Look at RGB image**\n",
        "\n",
        "Red, green, blue, 3 different color channels\n",
        "\n",
        "- So, in above image we have 3 color channels rgb.\n",
        "- we can think of this as a `stack of matrices`.\n",
        "- each layer/color channel represents the matrix.\n",
        "- as we already know scalar(rank-0 tensor), vectors(rank-1 tensor), and matrices(rank-2 tensors).\n",
        "\n",
        "### 4. 3D-tensors\n",
        "- So we this 3 dimentional data we call it **3D-tensor**\n",
        "- We can think stack of matrices as a 3D tensors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aHwmLEHpVYI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[[1., 2., 3.],[2., 3., 4.]],\n",
        "                  [[4., 5., 6.], [7., 8., 9.]]])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcRhdyW7_SfU",
        "outputId": "61de6008-903b-48e7-ace8-3bc617de1132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 2., 3.],\n",
              "         [2., 3., 4.]],\n",
              "\n",
              "        [[4., 5., 6.],\n",
              "         [7., 8., 9.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "So, when we use a.shape it returns 3 numbers\n",
        "- each number represents one dimention or one rank,\n",
        "- and then numbers represents the values in this dimention\n",
        "\"\"\"\n",
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfK5GNVrbALh",
        "outputId": "99d2ce48-e6f4-4783-a88c-00c174d28945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 4D(rank-4)tensor\n",
        "\n",
        "Going 1 step further we can also have a stack of multiple color images.\n",
        "\n",
        "this would add another dimention.\n",
        "\n",
        "And in this case we have 4 dimentional tensor or rank-4 tensor"
      ],
      "metadata": {
        "id": "GatqeFvcd5eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = torch.stack((a, a))\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g7TqJdcbQ72",
        "outputId": "6aa9b158-78ed-4475-a36f-26376627be49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1., 2., 3.],\n",
              "          [2., 3., 4.]],\n",
              "\n",
              "         [[4., 5., 6.],\n",
              "          [7., 8., 9.]]],\n",
              "\n",
              "\n",
              "        [[[1., 2., 3.],\n",
              "          [2., 3., 4.]],\n",
              "\n",
              "         [[4., 5., 6.],\n",
              "          [7., 8., 9.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xQLLJwngIMf",
        "outputId": "19d2862a-841a-49d0-cb9d-253fafc82b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it looks similar like numpy's array, now let's see how tensor is different than a numpy array.\n"
      ],
      "metadata": {
        "id": "8Dn6kBzQgWDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Tensor library with Array library\n",
        "\n",
        "- they are actually the same thing.\n",
        "- tensor library = array library\n",
        "- torch.tensor ~= numpy.array: torch.tensor is almost identical to numpy.array\n",
        "\n",
        "Difference\n",
        "\n",
        "|torch.tensor|\n",
        "|:-|\n",
        "|+ supports GPU computation|\n",
        "|+ Automatic differentiation support, very useful when training neural nets|\n",
        "\n",
        "### How tensors and arrays differe from regular python lists?\n",
        "\n",
        "**Python Lists**\n",
        "+ **Pros:** Can store heterogeous types (mix str, float, etc). you can store float, strings, and other objects mixed in a list.\n",
        "+ **Pros:** In python list we can easily remove or add items using `.append` or `.pop`\n",
        "+ **Cons:** while lists are easy to use and flexible, lists are very slow when it comes to numerical computation(that is the main motivation behind tensors)\n",
        "\n",
        "**Tensors**\n",
        "- Limitations of using tensors though is that all elements in tensors have to be the same type(eg. float, integer)\n",
        "- In contrast to lists, tensors also have a fixed size, so we can't easily add or remove If we want to have a larger tensor, we have to create new empty tensor with a larger size and copy over the all the elements and add the new elements to it\n",
        "\n",
        "this sounds like tensors are bad, However tensors have certain advantages over the lists which are extreamly useful for deep learning, which is heavily based on numerical computations.\n",
        "\n",
        "- tensors support wide variety of different computations.\n",
        "- numerical computations are fast"
      ],
      "metadata": {
        "id": "y0UMOFqZ49uJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Tensors In PyTorch\n",
        "\n",
        "1. `torch.tensor()`: Creating Tensors\n",
        "    - its most fundamental function. bcs that's how we create a tensors in PyTorch"
      ],
      "metadata": {
        "id": "e7XJ0SC6I9zK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1., 2., 3.])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54hdqrdagJYm",
        "outputId": "f1a56607-ec51-4e94-ebe3-ce8eaf6d0671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. `.shape` Checking Shape of Tensors\n",
        "    - tensor.shape to check the shape of the tensor.\n",
        "    - using `.shape` attribute we can check the no of elements in the tensor.\n",
        "    - in 2D tensor 1st no referes no of rows in tensor, and 2nd no referes no of columns in tensor.\n",
        "    - to check rank of the tensor count the no of no that are returned by `.shape`"
      ],
      "metadata": {
        "id": "4RiRWL7pJl_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1., 2., 3.], [3., 4., 5.]])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBrXSvpeJkSS",
        "outputId": "d62b82c5-7527-4590-b13b-d16819497b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3.],\n",
              "        [3., 4., 5.]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape # will get [2,3] tow numbers meaning its 2D tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lIrwXutLCGB",
        "outputId": "36f83377-017d-4cf2-c1c6-cd80e06466fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. `.ndim`: Checking the Rank/ Number of Dimentions\n",
        "    - use .ndim to check rank or dimention of the tensor"
      ],
      "metadata": {
        "id": "0F85ZAzXLNu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[[1., 2., 3.], [4., 5., 6.]], [[3., 4., 5.], [6., 7., 8.]]])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwwDi5AyLK0w",
        "outputId": "0ab2ebb3-af8c-4183-ee60-7d93a83a7456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 2., 3.],\n",
              "         [4., 5., 6.]],\n",
              "\n",
              "        [[3., 4., 5.],\n",
              "         [6., 7., 8.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.ndim # will get dimention of the tensor, in our case its 3D tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzMO-VUmL0_d",
        "outputId": "fd0d92c5-dd77-4617-f74e-44113c59d7de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. `.dtype`: Checking the Data type of Tensor\n",
        "    - as tensor can only store same type of data.\n",
        "    - we can see the datatype of the tensor.\n",
        "    - below it returns torch.float32, meaning 32 bit precision\n",
        "    - that's prefered precision in deep learning bcs of efficiency reasons"
      ],
      "metadata": {
        "id": "NlZoYESVMFVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1., 2., 3.], [5., 6., 7.]])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SClvJKvNL1yv",
        "outputId": "e601075f-b7f7-4009-9c8f-d851672b83e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3.],\n",
              "        [5., 6., 7.]])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.dtype # will get data type of tensor that is float32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0vK_tEkMj2v",
        "outputId": "e0ad195a-0e8d-4e07-c085-78f013412dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1,2,3],[4,5,6]])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf9Psh6IMlUH",
        "outputId": "129a4d66-cd76-4e5c-e51a-8a831d68cbef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.dtype # int datatype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK5xiyrTNWkR",
        "outputId": "59628474-2ec9-4a1f-b6ae-f16e4c7fe099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. `torch.from_numpy(np_array)` Creating Tensor from Numpy Array\n",
        "    - torch has the `from_numpy()` function which lets us convert Numpy array directly into Tensor.\n",
        "    - can also call `.tensor` on numpy array, but that would create a copy in memory.\n",
        "    - using from_numpy() function it will use the same memory as the numpy array.\n",
        "    - since python uses 64bit precision bydefault, the converted tensor from numpy will be the same 64bit.\n"
      ],
      "metadata": {
        "id": "TM6UDSv7itqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "gQ-vp6CLNYfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_array = np.array([1., 2., 3.]) # creating numpy array\n",
        "print(f\"Numpy array: {np_array}\")\n",
        "m2 = torch.from_numpy(np_array) # creating tensor from numpy array, its dtype will be same as numpy's that is 64bit,\n",
        "# its 64 because its default type in numpy\n",
        "m2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojOeSKupsC4W",
        "outputId": "a21d4370-3bc1-4859-8686-40b943a8dbb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy array: [1. 2. 3.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. `tensor_obj.to(new_dtype)` Change the dtype"
      ],
      "metadata": {
        "id": "CgV57S2Ws1Oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m2 # currently its float64 dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfrOD4oTsbyD",
        "outputId": "361a9e64-24e9-4e43-f8aa-315d196b98e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m2 = m2.to(torch.float32) # changing 64bit datatype to 32\n",
        "m2.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyODemU2s7oI",
        "outputId": "f15a9b3a-cc3b-471d-e71b-b065614c7347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. `.device` Checking the device Type\n",
        "    - tensors also have a `.device` attribute, that show us where on our computer the tensor is located.\n",
        "    - So usually it will return CPU which means tensor is on CPU's memory.\n",
        "    - Later will see how to transfer tensors to the GPU which can be very useful for deep learnig and accelerating training."
      ],
      "metadata": {
        "id": "lc5vsaADxaFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m2.device # currently its on cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFQKLt5NwPoS",
        "outputId": "b14a1aa2-9e4e-4dee-ca31-7c6e65322c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Changing Shape of a Tensor\n",
        "    - Using `.view()` function we can change the shape of the tensor\n",
        "    - `.view(-1,2)` here `-1` means it will automatically decide the no of rows/ dimention. here we're saying that we want no of columns 2, and because there's only one way it can have 2 columns it will put 3 rows automatically for us.\n",
        "    - `.view(2, -1)` and if we use `-1` in column placeholder, that means it will automatically determine that dimention"
      ],
      "metadata": {
        "id": "2cy2M0q2yppA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.shape)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VENnmoXnyjCY",
        "outputId": "bfeba320-2a40-42c3-8559-ee90db7065f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.view(3,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0GzDL3fy75U",
        "outputId": "ba637f11-c177-4fd7-b75a-66ea1630614e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4],\n",
              "        [5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.view(-1,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71Qr_WvezIig",
        "outputId": "37618ba2-24f3-4ea8-bd3d-0dafffa4b19b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Transposing a Matrix\n",
        "    - There's a concept of transposing a tensor like matrix transpose.\n",
        "    - Transposing a matrix meaning flipping it along its diagonal. like row values becomes column and column becomes rows."
      ],
      "metadata": {
        "id": "KYzj6uLa6ZtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n",
        "m # before transposing a matrix its shape is 2,3. 2 rows and 3 columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LjVrMtszsXm",
        "outputId": "faa0c1e2-c1f5-4853-bf26-11de59922f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3.],\n",
              "        [4., 5., 6.]])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.T # applying transpose operation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emOoCXfK6e6G",
        "outputId": "f1a3ab72-2d46-4d39-fc3f-a4f0fbe87a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 4.],\n",
              "        [2., 5.],\n",
              "        [3., 6.]])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Multiplying Matrices\n",
        "    - This is something that we do a lot in deeplearning.\n",
        "    - Can use concepts from linear algebra to make code more efficient and faster"
      ],
      "metadata": {
        "id": "-qzAfMj6O2SM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improving Code with Linear Algebra\n",
        "\n",
        "- From for loop to dot product\n",
        "- As we already know basics of tensors, let's see how to use concepts from linear algebra to encode certain things like the weighted sum more efficiently.\n",
        "\n",
        "**Weighted Sum**\n",
        "\n",
        "- its computed by multiplying inputs with the weights\n",
        "\n",
        "`TODO: Image of neuron`\n",
        "\n",
        "$$ b + x_1w_1 + x_2w_2 $$\n",
        "\n",
        "- we do this computation for each individual training example.\n",
        "\n",
        "1 training example with 2 features values\n",
        "\n",
        "$$ z = b + x_1 * w_1 + x_2 * w_2 $$\n",
        "\n",
        "General form\n",
        "\n",
        "$$ z = b + ∑_{j=1}^m x_jw_j $$\n",
        "\n",
        "- we can express weighted sum is dot product between `x` and `w` two vectors. if `X`and `W` are vectors.\n",
        "\n",
        "$$  = b + XW $$\n",
        "\n",
        "- can use concept from linear algebra called dot product to compute weighted sum\n"
      ],
      "metadata": {
        "id": "u2a7ZhAVsuzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dot product or multiplication of vectors in plain Python\n",
        "b = 0\n",
        "x = [1.2, 2.2]\n",
        "w = [3.3, 4.3]\n",
        "\n",
        "output = b\n",
        "for xi, wi in zip(x,w):\n",
        "  output += xi * wi\n",
        "\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQOM_D7J6noc",
        "outputId": "292e0b93-ab9e-4bfa-bf60-39679809425d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.42"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dot product between 2 vectors\n",
        "\n",
        "$$ z = b + X^TW $$\n",
        "\n",
        "Let's see step by step\n",
        "\n",
        "`X` is a vector from 1 to m. and `W` is the vector from 1 to m.\n",
        "\n",
        "$$ X = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ .\\\\.\\\\.\\\\x_m\n",
        "\\end{bmatrix} W = \\begin{bmatrix}w_1 \\\\ w_2 \\\\ .\\\\.\\\\.\\\\w_m\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "- now adding a transpose operation here. It will become handy during matrix multiplication.\n",
        "- Transpose operation is transposing the column vector `X` into row vector `X^T` x transpose.\n",
        "\n",
        "### `.dot()` to compute the dot product of two vectors\n",
        "\n",
        "- It's much simpler and compact than venila python using for loop.\n",
        "- Biggest advantage of using `.dot()` dot product is its **much much faster**.\n",
        "- we can use `.dot()` dot product to improve code efficiency.\n",
        "- question is how much faster? let's do benchmark"
      ],
      "metadata": {
        "id": "2i2Sl5uUe_Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dot product of two vectors using .dot using PyTorch\n",
        "b = torch.tensor(0.)\n",
        "x = torch.tensor([1.2, 2.2])\n",
        "w = torch.tensor([3.3, 4.3])\n",
        "\n",
        "x.dot(w) + b  # much simpler than the plain python. It's more compact than venila python using for loop\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQlM27AIeVuP",
        "outputId": "72263690-82b0-4a77-f428-484c7877f205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(13.4200)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FXRWz7qkvek_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarking Venila Python List and PyTorch dot product\n"
      ],
      "metadata": {
        "id": "rRTGzIGj3vb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plain_python(x, w, b):\n",
        "  out = b\n",
        "  for xi, wi in zip(x,w):\n",
        "    out += xi * wi\n",
        "  return out\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(123)\n",
        "\n",
        "b = 0\n",
        "x = [random.random() for _ in range(1000)]\n",
        "w = [random.random() for _ in range(1000)]\n",
        "\n",
        "\n",
        "%timeit plain_python(x, w, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzdF6PAVw2Ds",
        "outputId": "0503971d-dce0-4fcb-cb8e-554087e94d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123 µs ± 53.2 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In PyTorch\n",
        "def pytorch_dot(x, w, b):\n",
        "  return x.dot(w) + b # dot product using pytorch\n",
        "\n",
        "# First convert python list to tensor\n",
        "b = torch.tensor(b)\n",
        "x = torch.tensor(x)\n",
        "w = torch.tensor(w)\n",
        "\n",
        "%timeit pytorch_dot(x, w, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAGr-it4x7JO",
        "outputId": "535c292d-d230-4183-ad1f-4d91c8d1d22c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.37 µs ± 1.36 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing runtime of plain python with pytorch tensor there is 25X speedup! in pytorch, which is huge.\n",
        "\n",
        "so that's a good reason to replace python for loop with pytorch dot product.\n",
        "\n",
        "So far it was about computation/weighted sum of single vector/training example. However in practice in deep learning we usually work with large dataset. and for that we use matrix multiplication."
      ],
      "metadata": {
        "id": "b_Nqmi8V4C0c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yf89T72nyg4k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}