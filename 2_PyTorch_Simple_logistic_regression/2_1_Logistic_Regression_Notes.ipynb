{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Model Training Pipeline Using PyTorch: Logistic Regression\n",
        "\n",
        "**AIM**\n",
        "\n",
        "- Understanding training algorithm behind neural networks\n",
        "- Looking at PyTorch's automatic differenciation capabilities\n",
        "- **Train Models in PyTorch**: defining dataset class, dataloader, defining model and its forward pass, training a model, and finally evaluating the model.\n",
        "- And later implementing deeper neural networks.\n"
      ],
      "metadata": {
        "id": "6CWXLTGMLJiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification: Using Logistic Regression\n",
        "\n",
        "**Linear Regression**: This would also be a single layer neural networks.\n",
        "\n",
        "**Classification (Perceptron)**: This is another form of NN.\n",
        "\n",
        "**General Structure of Single Layer NN**\n",
        "![](https://raw.githubusercontent.com/srkds/PyTorch-Lightning-DL/main/2_PyTorch_Simple_logistic_regression/assets/slp.jpg)\n",
        "\n",
        "\n",
        "Here are some general annotations like `input weights`, `bias unit`, `weighted sum`, `activation`, and then finally the output. These structure maps to all three Linear Regression, Perceptron and Logistic Regression.\n",
        "\n",
        "`Linear Regression`: Its a single layer NN where the `output` is the `continues value`.\n",
        "\n",
        "`Perceptron`: Its the same structure but here the `output` will be `class 0/1`.\n",
        "\n",
        "`Logistic Regression`: Its also similar to `Perceptron`, the `output` will be `label 0/1`\n",
        "\n",
        "> ğŸ’¡ Note: Logistic regression may look like a regression method but its actually a classifier.\n",
        "\n",
        "**Threshold function in Logistic regression**: There is threshold function right after activation function which checks weather the probability score from activation function is `> 0.5` or not. If its `> 0.5` it will `output class label 1` and if its smaller or = 0.5 then will `output class label 0`.\n",
        "\n",
        "**Activation function: Logistic Regression**\n",
        "\n",
        "This is also called **Sigmoid function** which is non linear activation function.\n",
        "$$ f(z) = \\frac{1}{1 + e^{-z}} $$\n",
        "\n",
        "![](https://raw.githubusercontent.com/srkds/PyTorch-Lightning-DL/main/2_PyTorch_Simple_logistic_regression/assets/af.jpg)\n",
        "\n",
        "Left most side is linear function. Middle is threshold function for perceptron. and right most side is the non linear sigmoid activation function.\n",
        "\n",
        "**Sigmoid**: on the x-axis we have input values, on the y-axis we have output values. We can see the output range on y-axis is between 0 and 1.\n",
        "\n",
        "Look at input point x = 0, we can see output point y = 0.5.\n",
        "\n",
        "so if we make the input greater > 0 positive then the sigmoid output value will be greater than 0.5 and vice versa.\n",
        "\n",
        "![](https://raw.githubusercontent.com/srkds/PyTorch-Lightning-DL/main/2_PyTorch_Simple_logistic_regression/assets/tlsf.jpg)\n",
        "\n",
        "So in logistic regression model we pass weighted sum value to the logistic activation function and then we threshold unit which checks whether the logistic function output is greater > 0.5 or not. If its > 0.5 then label it as 1 and if its < 0.5 then its label as 0.\n",
        "\n",
        "- Logistic regression classifier will always converge in contrary of the linear classifier.\n",
        "\n",
        "**Loss Function**\n",
        "\n",
        "- Use to train the neural networks.\n",
        "- For training we only need weighted sum, activation function, and true class label and loss function.\n",
        "- Here, the goal is to minimize the loss.\n",
        "- Name of loss function is negative log likelihood or binary cross entropy.\n",
        "\n",
        "![](https://raw.githubusercontent.com/srkds/PyTorch-Lightning-DL/main/2_PyTorch_Simple_logistic_regression/assets/training.jpg)"
      ],
      "metadata": {
        "id": "eMBbm_HQMyN5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNvFvCosK5_i"
      },
      "outputs": [],
      "source": []
    }
  ]
}