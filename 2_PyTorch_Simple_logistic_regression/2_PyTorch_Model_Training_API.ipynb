{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Model Training API\n",
        "\n",
        "## Define Model in PyTorch\n",
        "\n",
        "General Structure of defining a model in PyTorch. This is something that we would do for any type of model, whether its logistic regression or any other deep neural networks.\n",
        "\n",
        "In PyTorch we **define models as classes** and that **inherits the class** `torch.nn.Module` which will give us certain sets of properties that is helpful for later in model training\n",
        "\n",
        "**__init__ constructor**: here we would define model parameters, like layers and all the structure. Here we define the structure of model like all layers eg: linear, attention etc. defining its input and output sizes etc.\n",
        "\n",
        "**forward() method**: here we will pass the data point inputs and define how the does the predictions, or model copmutations will take place here.\n",
        "\n",
        "```py\n",
        "import torch\n",
        "\n",
        "class MyModelName(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    # define model parameters or layers or modules\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # define how model produces outputs\n",
        "    return outputs\n",
        "```\n",
        "\n",
        "## Train Model\n",
        "Suppose we implemented `MyModelName` model class. Then we initialize the model using `model = MyModelName()`.\n",
        "\n",
        "Then we **initialize the optimizer**, here we are using stochastic gradient descent, so we define/initialize it like `optimizer = torch.optim.SGD(...)`\n",
        "\n",
        "**Epoch** one single pass on whole training dataset, here we are iterating over training epoch.\n",
        "\n",
        "\n",
        "for each training epoch here we compute the model output and compute the loss\n",
        "\n",
        "**train_dataloader** which will produce mini batches for us.\n",
        "\n",
        "after computing a loss, 1st we call `optimizer.zero_grad()` then we calculate the gradient of the each parameters using `loss.backward()` and finally calling `optimizer.step()` to update the model parameters.\n",
        "\n",
        "**Why optimizer.zero_grad()?**: its reseting the gradients from previous iterations. if we omit this, gradients will be accumulated, which we usually dont want. for more clarity checkout micrograd autograd engine.\n",
        "\n",
        "```py\n",
        "model = MyModelName() # initialize the model\n",
        "optimizer = torch.optim.SGD(...) # initialize optimizer\n",
        "\n",
        "for epoch in range(no_of_epochs):\n",
        "  for x, y in train_dataloader:\n",
        "    # forward pass\n",
        "    outputs = model(x)\n",
        "    loss = loss_fn(outputs, y)\n",
        "\n",
        "    # backward pass (backpropagation)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() # computes the gradients\n",
        "\n",
        "    # update the model parameters\n",
        "    optimizer.step() # and it updates the model parameters\n",
        "```"
      ],
      "metadata": {
        "id": "yFKD-ZpWaSO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks Layer in PyTorch\n",
        "\n",
        "`torch.nn.Linear`: its a neural network layer that computes the weighted sum\n",
        "- it assigns weight vector and bias for us."
      ],
      "metadata": {
        "id": "nKrDPbLIJx48"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QftmoT2nXrBR"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123) # for reproducablility every time getting same result, in real-world its not required\n",
        "\n",
        "# below is the linear layer, or layer to compute weighted sum\n",
        "# with 2 input features x1, x2\n",
        "# and 1 output feature, meaning 1 weighted sum\n",
        "# out feature 1 meaning theres only 1 neuron in layer and it has 3 inputs\n",
        "# if we increase out features to 2 meaning it is having 2 neurons and have same 3 inputs to each perceptron.\n",
        "linear = torch.nn.Linear(in_features=3, out_features=1) # this will create a weights and bias"
      ],
      "metadata": {
        "id": "tbd8Q0qenhbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see weights and biases are initialized with small numbers than 0. That's because its best practice to use small random number when we train neural network models."
      ],
      "metadata": {
        "id": "B98InRrWqkfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear.weight # it contains 3 features because we have defined 3 in features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x31ZtmitqMvZ",
        "outputId": "24f0b4d7-3549-4009-b8de-49c3781011a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.2354,  0.0191, -0.2867]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear.bias # checking the bias unit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gin7O6pqO3s",
        "outputId": "608939ba-2a2b-4396-90fe-79f67d7c048f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([0.2177], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing weighted sum\n",
        "\n",
        "$$z = XW^{T} + b$$\n",
        "\n",
        "- Its weighted sum between X and W + bias unit\n",
        "- if we have a single training example `X` is a vector, and we have weight vector `w` consisting of same no of weights as the input consists of no of features.\n",
        "- We can express this via dot product or matrix multiplication"
      ],
      "metadata": {
        "id": "ZRhlFxl2s2Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# single training example\n",
        "x = torch.tensor([[1.2, 0.5]])\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z67Fywi414Di",
        "outputId": "bcc22452-a0c1-48f5-ba8d-cec2e5c92104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.2000, 0.5000]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear = torch.nn.Linear(in_features=2, out_features=1)"
      ],
      "metadata": {
        "id": "DxMV0kuY14At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = linear.weight.detach() # getting weight vector from linear layer or computation graph\n",
        "b = linear.bias.detach()\n",
        "print(f'shape of x is {x.shape}')\n",
        "print(f'shape of w is {w.shape}')\n",
        "print(f'shape of w transpose is {w.T.shape}')\n",
        "z = x.matmul(w.T) + b\n",
        "\n",
        "print(f'z is {z}')\n",
        "print(f'shape of z {z.shape}')\n",
        "\n",
        "# NOTE: this is not the way we use the linear layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t7sHBGjqYJa",
        "outputId": "5d6bb1c4-1688-4f5d-ab27-ba1feececda6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of x is torch.Size([1, 2])\n",
            "shape of w is torch.Size([1, 2])\n",
            "shape of w transpose is torch.Size([2, 1])\n",
            "z is tensor([[-0.9778]])\n",
            "shape of z torch.Size([1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here is the nice way to use linear layer\n",
        "z = linear(x) # directly passing the input to linear layer\n",
        "z # will get same output as above"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2Yuwu8-2HGG",
        "outputId": "e97e1bf1-8f2e-4af9-ed88-d2faf3d88587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.9778]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How does linear layer handle mini batches?\n"
      ],
      "metadata": {
        "id": "hZRKsZi4_8xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# suppose here, we have mini batch consisting of 3 training examples\n",
        "x = torch.tensor([[1.2, 0.5],\n",
        "                  [0.7, 0.8],\n",
        "                  [0.1, 0.2]]) # 3 training examples\n",
        "\n",
        "print(x) # 3 training examples and each has 2 features, column represents features and rows are no of examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9HS3sk-_wUD",
        "outputId": "79735aea-ad86-48ef-8e6b-94c06cb1c262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.2000, 0.5000],\n",
            "        [0.7000, 0.8000],\n",
            "        [0.1000, 0.2000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOCE4pcMBQ6X",
        "outputId": "45630a96-649b-4243-9b69-873725523b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.6025],\n",
              "        [ 0.5183]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to compute the net input of weighted sum, we would do matmul on x and on transpose of w weight vector\n",
        "z = x.matmul(w.T) + b\n",
        "\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWd1Ce2ZAdnP",
        "outputId": "4757dc6c-03c0-45ce-e884-f21e5ade4d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.9778],\n",
            "        [-0.5210],\n",
            "        [-0.4705]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is something that linear layer takes care of we dont have to worry\n",
        "# just call linear on input, and if input is batch of multiple training examples every thing will be taken careof\n",
        "z = linear(x)\n",
        "z # same result as above"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF_8K5tEBOv6",
        "outputId": "bc21ebe9-c8b3-45a7-c93f-2645c5fb02cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.9778],\n",
              "        [-0.5210],\n",
              "        [-0.4705]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Logistic Regression using PyTorch\n",
        "\n",
        "## Defining the Class"
      ],
      "metadata": {
        "id": "IdHKftd8uaUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # define class name and inherit torch.nn.Module\n",
        "class LogisticRegression(torch.nn.Module):\n",
        "\n",
        "  # Define constructor for defining model layer or structures\n",
        "  def __init__(self, no_of_inputs):\n",
        "    super().__init__() # super call will call init function of Module, that is necessary to import all the important thigns to make training work.\n",
        "    self.linear = torch.nn.Linear(no_of_inputs, 1) # Initializing linear layer\n",
        "\n",
        "  def forward(self, x):\n",
        "    logits = self.linear(x) # logits is a technical term for weighted sum.\n",
        "    probs = torch.sigmoid(logits) # computing probability using sigmoid function\n",
        "\n",
        "    return probs\n"
      ],
      "metadata": {
        "id": "uChTh-XCBpmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "# Initialize the LogisticRegression Model\n",
        "\n",
        "model = LogisticRegression(2)"
      ],
      "metadata": {
        "id": "Lt3eKfU0xZ5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.1, 2.1])\n",
        "\n",
        "with torch.no_grad(): # disabling the computation graph of model, because here we dont need to calculate gradient,\n",
        "# its best practice to save computer memory and make model prediction more faster\n",
        "  probas = model(x)\n",
        "probas # 0.4033 is the probability"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QDtc7EtxjI5",
        "outputId": "461f46d3-d884-4d29-8111-548c473a5d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4033])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Dataloader class\n",
        "\n",
        "For this we need to import a `Dataset`, and `DataLoader` class from `torch.utils.data`.\n"
      ],
      "metadata": {
        "id": "1AOE0dccEPiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### MyDataset class\n",
        "# which is a child class of Dataset parent class\n",
        "# It takes X features and y labels\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.features = torch.tensor(X, dtype=torch.float32)\n",
        "    self.labels = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # this function will fetch individual training example/record from X/features\n",
        "    # x is training features and y is labels\n",
        "    x = self.features[index]\n",
        "    y = self.labels[index]\n",
        "\n",
        "    return x, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.labels.shape[0]\n"
      ],
      "metadata": {
        "id": "LZ77_TAyy0ZX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}